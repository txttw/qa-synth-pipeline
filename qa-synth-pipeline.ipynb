{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dccef079-56ea-47ab-87c4-410299c0db40",
   "metadata": {},
   "source": [
    "# **SVO-triplet constrained, unsupervised QA synthesis pipeline with hybrid dual-scale retrieval for technical knowledge indexing**\n",
    "### **Overview**\n",
    "This system transforms unstructured technical documentation (product specs, expert \n",
    "articles, manuals) into a query-optimized knowledge base. By integrating linguistic \n",
    "structure extraction, constrained generation, and multi-strategy retrieval, it enables \n",
    "precise technical question answering while minimizing computational costs. \n",
    "\n",
    "### **Target application**\n",
    "Ideal for knowledge bases, chat-bots, and product search.\n",
    "\n",
    "### **Key Components**\n",
    "#### Semantic-aware chunking\n",
    "- After coarse rule based chunking with overlap an LLM splits the larger texts into smaller coherent chunks, preserving technical context and logical flow.\n",
    "- Benefit: Maintains explanatory relationships between concepts within chunks, avoiding arbitrary fragmentation.\n",
    "\n",
    "#### SVO triplet extraction\n",
    "- Each sentence in a chunk is processed to extract Subject-Verb-Object (SVO) triplets (e.g., \"The algorithm [S] optimizes [V] performance [O]\").\n",
    "- Design decision: Triplets enforce structured representation of core relationships, reducing ambiguity and halucination.\n",
    "\n",
    "#### SVO-constrained QA generation\n",
    "- An LLM generates QA pairs using SVO triplets as constraints, with the full chunk as context.\n",
    "- Key Innovations:\n",
    "    - Cost efficiency: SVO direction allows smaller/cheaper LLMs (e.g., 7B-parameter models) without sacrificing precision.\n",
    "    - Detail preservation: Questions target granular technical elements (e.g., \"What does the algorithm optimize?\") while retaining broader context.\n",
    "\n",
    "#### Validation & Filtering\n",
    "- A “roberta” based model distilled for Extractive QA scores each generated pair:\n",
    "    - Answer relevance to the question.\n",
    "    - Contextual alignment with the source chunk.\n",
    "- Low-scoring QAs are discarded via a threshold.\n",
    "- Benefit: Ensures factual accuracy and eliminates hallucinated content.\n",
    "\n",
    "#### Redundancy reduction\n",
    "- Question embeddings are clustered using Affinity Propagation to identify semantic redundancy.\n",
    "- Design decision: Affinity Propagation adapts to unknown cluster counts, automatically retaining representative questions.\n",
    "\n",
    "#### Dual-scale Embedding\n",
    "- Final QA pairs are embedded jointly (question + answer as one vector). for precise, detail-oriented queries.\n",
    "- Whole chunks for broad-context retrieval.\n",
    "\n",
    "#### Hybrid embedding:\n",
    "- Dense retrieval: Semantic matching via dense embedding\n",
    "- Sparse retrieval: Term-attention based matching with IDF calculation for keyword relevant queries\n",
    "- Late Interaction (ColBERT): Contextualized token-level relevance scoring\n",
    "\n",
    "### **Benefits**\n",
    "#### Cost-effective scalability\n",
    "- Indexing: Smaller LLMs handle SVO-triplet-constrained generation, reducing indexing costs vs. large-model approaches.\n",
    "- Retrieval: Only uses embedding, vector-search and a re-ranking. No LLM or NER model.\n",
    "\n",
    "#### Precision-recall balance\n",
    "- SVO anchoring captures atomic details, while chunk context supports conceptual queries.\n",
    "\n",
    "#### High signal-to-noise ratio\n",
    "- Validation + clustering typically removes 30-40% of generated QAs, retaining only novel, high-value pairs.\n",
    "\n",
    "#### Optimized retrieval\n",
    "- Joint QA embeddings enable direct matching of questions to answer-bearing content.\n",
    "- Dual embeddings support both specific fact lookup and exploratory searches.\n",
    "- Hybrid (dense+sparse) embedding increase retrieval precision\n",
    "\n",
    "### **Requirements**\n",
    "1. This notebook uses Cloudflare Workers AI for different models. It also runs some model locally that require some free memory (1-2GB)\n",
    "2. Qdrant is used as a Vector DB for testing retrieval. A Qdrant cluster is required. Free managed cluster is available that is more than enough for testing.\n",
    "\n",
    "**Important**: Text generation nodel needs to support structured output (JSON). Even if it supports sometimes it fails to generate a propper JSON output. Therefore the code is written that long running (multiple) generations can be resumed. e.g. question generation\n",
    "\n",
    "*Huggingface inference (optional) is implemented but the 'json_schema' for output_format is limited and this notebook heavily relies on that feature.\n",
    "It could still work with 'json_object' as output format but not tested. HF would be an option to test RAG results with a wide range of models.*\n",
    "\n",
    "### **Important Notes**\n",
    "- This notebook uses services. They should fit to free tiers but always verify pricing before providing API keys.\n",
    "- JSON files are provided as data source. There is code to load them and skip long running task.\n",
    "- The code has been made for demonstrational purposes. Validation, error handling, etc. omitted for better understanding. It is not made for production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaad141-2000-4cfb-8987-bc3d06589dd7",
   "metadata": {},
   "source": [
    "### **Use a virtual environment**\n",
    "#### Steps to create with conda:\n",
    "1. Create myenv  \n",
    "`conda create -n myenv`  \n",
    "*or with python version*  \n",
    "`conda create -n myenv python=3.12`\n",
    "2. Activate  \n",
    "`conda activate myenv`\n",
    "3. Install ipykernel  \n",
    "`conda install ipykernel`\n",
    "4. Add venv to Jupyter with name myenv  \n",
    "`python -m ipykernel install --user --name=myenv`\n",
    "\n",
    "### **Installation**\n",
    "Install packages from requirements.txt (recommended)  \n",
    "`pip install -r requirements.txt`  \n",
    "Or use this command  \n",
    "`pip install haystack-ai dotenv huggingface_hub cloudflare ipywidgets spacy qdrant-client fastembed accelerate`  \n",
    "Install claucy  \n",
    "`pip install git+https://github.com/mmxgn/spacy-clausie.git`  \n",
    "Download the model for spacy  \n",
    "`python -m spacy download en_core_web_trf`  \n",
    "\n",
    "### **API Access**\n",
    "Set the following environment variables to use services  \n",
    "Cloudflare workers AI  \n",
    "`CLOUDFLARE_ACCOUNT_ID` - Cloudflare account ID  \n",
    "`CLOUDFLARE_API_TOKEN` - Cloudflare API Token  \n",
    "Qdrant  \n",
    "`QDRANT_URL` - Qdrant cluster URL  \n",
    "`QDRANT_API_KEY` - Qdrant API key  \n",
    "Hugging Face (optional)  \n",
    "`HF_TOKEN` - Hugging Face API Token   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0b41e-f7d2-4544-9f79-a9e83dc82cde",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "679e924a-8ea2-4090-b54e-c8f749b37867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# load environment\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed3e1fe-9307-4ce1-bccd-c7c32b4c4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "import spacy\n",
    "import claucy\n",
    "import json\n",
    "\n",
    "# This will load the model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "claucy.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da526870-6400-41e7-976c-8177b3e8de30",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f18521-03b7-4901-898e-e1a01a6e0d15",
   "metadata": {},
   "source": [
    "### Model output schema\n",
    "We use pydantic to define the schema and pass to the inference endpoint as JSON schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc4833-0f38-4b32-86cc-2ea4705479bf",
   "metadata": {},
   "source": [
    "#### Utility functions to generate JSON schema from pydantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c230d8a8-33a1-45ce-86cb-7c4dd89a61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field, BaseModel\n",
    "\n",
    "def replace_value_in_dict(item, original_schema):\n",
    "    if isinstance(item, list):\n",
    "        return [replace_value_in_dict(i, original_schema) for i in item]\n",
    "    elif isinstance(item, dict):\n",
    "        if '$ref' in list(item.keys()):\n",
    "            definitions = item['$ref'][2:].split('/')\n",
    "            defs = original_schema[definitions[0]]\n",
    "            return defs[definitions[1]]\n",
    "        else:\n",
    "            return { key: replace_value_in_dict(i, original_schema) for key, i in item.items()}\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "def model_to_json_schema(model: type[BaseModel], max_depth = 20):\n",
    "    model_schema = model.model_json_schema()\n",
    "    json_schema = model_schema\n",
    "    i = 0\n",
    "    while '$ref' in json.dumps(json_schema) and i < max_depth:\n",
    "        json_schema = replace_value_in_dict(json_schema, json_schema)\n",
    "        i += 1\n",
    "\n",
    "    if \"$defs\" in json_schema:\n",
    "        del json_schema['$defs']\n",
    "\n",
    "    return json_schema\n",
    "\n",
    "def model_to_cf_response_format(model: type[BaseModel]):\n",
    "    return json.dumps({\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": model_to_json_schema(model)\n",
    "    })\n",
    "\n",
    "def model_to_hf_response_format(model: type[BaseModel]):\n",
    "    return {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"value\": model_to_json_schema(model)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f259caa-611b-40e1-a0cf-0bb09cd003a5",
   "metadata": {},
   "source": [
    "### Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7968bfb-1df5-41d9-8ad2-db33721348b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudflare import Cloudflare\n",
    "from huggingface_hub import InferenceClient\n",
    "import requests\n",
    "\n",
    "class InferenceProviderBase(ABC):\n",
    "    @abstractmethod\n",
    "    def completition(self, model: str, messages: dict, output_format = None):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class HFInferenceProvider(InferenceProviderBase):\n",
    "    def __init__(self):\n",
    "        self.client = InferenceClient(\n",
    "            provider=\"auto\",\n",
    "            api_key=os.getenv('HF_TOKEN'),\n",
    "        )\n",
    "    def completition(self, model, messages, response_schema=None):        \n",
    "        \"\"\"\n",
    "        IGNORED: response_schema is ignored    \n",
    "        Tried (directly the HTTP endpoint) a few inference endpoints this notebook uses and many does not support json_schema validation:\n",
    "        Response:\n",
    "        {\n",
    "            \"code\": 400,\n",
    "            \"reason\": \"INVALID_REQUEST_BODY\",\n",
    "            \"message\": \"model features structured outputs not support\",\n",
    "            \"metadata\": {}\n",
    "        }        \n",
    "\n",
    "        We will use: response_format={\"type\": \"json_object\"}\n",
    "        It produces a JSON output but a schema is not enforced\n",
    "        So parsing might fail\n",
    "        Cloudflare has better support for JSON schema validated inference endpoints\n",
    "        \"\"\"\n",
    "        #response_format = model_to_hf_response_format(response_schema) if response_schema else None\n",
    "        res = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"} if response_schema else None\n",
    "        )\n",
    "        # This might fail bc of malformed json\n",
    "        return json.loads(res.choices[0].message.content)\n",
    "\n",
    "class CFInferenceProvider(InferenceProviderBase):\n",
    "    def __init__(self):\n",
    "        self.client = Cloudflare(api_token=os.getenv('CLOUDFLARE_API_TOKEN'))\n",
    "        self.account_id = os.getenv('CLOUDFLARE_ACCOUNT_ID')\n",
    "    def completition(self, model, messages, response_schema=None, max_tokens=3000):\n",
    "        response_format = model_to_cf_response_format(response_schema) if response_schema else None        \n",
    "        result = self.client.ai.run(\n",
    "            model,\n",
    "            account_id=self.account_id,\n",
    "            messages=messages,\n",
    "            response_format=response_format,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        # sometimes this can throw if the model output is not well formed therefore the parsing fails\n",
    "        return json.loads(result['response']) if response_schema else result['response']\n",
    "    def embedding(self, model, texts):        \n",
    "        result = self.client.ai.run(\n",
    "            model,\n",
    "            account_id=self.account_id,\n",
    "            text=texts\n",
    "        )\n",
    "        if 'data' not in result:\n",
    "            raise RuntimeError('Unable to embedd data')\n",
    "        return result['data']\n",
    "    def rerank(self, model, query:str, contexts: list[str], top_k=10):\n",
    "        response = requests.post(\n",
    "            f\"https://api.cloudflare.com/client/v4/accounts/{os.getenv('CLOUDFLARE_ACCOUNT_ID')}/ai/run/{model}\",\n",
    "            headers={\"Authorization\": f\"Bearer {os.getenv('CLOUDFLARE_API_TOKEN')}\"},\n",
    "            json={\n",
    "                \"query\": query,\n",
    "                \"contexts\": contexts,\n",
    "                \"top_k\": top_k\n",
    "            }\n",
    "        )\n",
    "        json_res = response.json()\n",
    "        return json_res[\"result\"][\"response\"] if \"result\" in json_res and \"response\" in json_res[\"result\"] else []        \n",
    "        return [ { \"content\":contexts[result[\"id\"]][\"text\"], \"score\":result[\"score\"] } for result in results ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb38d20-1789-4eac-b164-439adaa48f76",
   "metadata": {},
   "source": [
    "### Constrained question generation prompt\n",
    "This prompt needs to be adjusted to the application domain.\n",
    "We focus on product descriptions, expert articles, product reviews so generating primarily How and What questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "19fa79e1-3c6f-431a-af98-2aff0fa29b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "_genq_system_prompt = \"\"\"\"\n",
    "You are generating context relevant question - amswer (QA) pairs using specified constraints: subjects, verbs, direct or indirect objects and complement.\n",
    "\n",
    "Ensure:\n",
    "   - Grammatically correct question format\n",
    "   - Preserve semantics, logical relationships and key technical terms\n",
    "   - The context is broader for more information, but concentrate on the constraints (subject, verb, object).\n",
    "   - Use primarily \"How\" or \"What\" or semantically similar words/phrases for questions.\n",
    "   - Replace the pronoun if possible (they -> CPU and GPU) and always use a noun or noun phrase (e.g. Motherboard, sensor, etc.)\n",
    "   - Use the correct form of the constrains (SVO triplets) in the questions (e.g. they -> them)\n",
    "   - Answer the questions using the provided context. Do not invent, add or remove meaning.\n",
    "   - Generate multiple QA pairs\n",
    "   - Do not add comments or notes, just generate the QA pairs\n",
    "   - Follow the specified JSON output format\n",
    "\n",
    "Example:\n",
    "Create questions using:\n",
    "- verb: have\n",
    "- subject: They\n",
    "- direct objects: different roles\n",
    "\n",
    "Context:\n",
    "The main difference between a CPU and a GPU is their purpose in a computer system. They have different roles depending on the system.\n",
    "\n",
    "---\n",
    "Output:\n",
    "{\n",
    "    \"qa_pairs\": [\n",
    "        {\n",
    "            \"q\": \"What do they have depending on the system?\",\n",
    "            \"a\": \"CPU and a GPU have different roles depending on the system\"\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"What roles do they have\",\n",
    "            \"a\": \"CPU and a GPU have different roles depending on the system.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "_genq_input_prompt = \"\"\"\n",
    "Create questions using:\n",
    "{constraints}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def _handle_plural_form(cnt:int, word: str):\n",
    "    # In this constrained case we can simply add 's'\n",
    "    return f'{word}s' if cnt > 1 else word\n",
    "\n",
    "def _create_constraint_line(terms: list[str], name: str):\n",
    "    l = len(terms)\n",
    "    return f\"- {_handle_plural_form(l, name)}: {', '.join(terms)}\\n\" if l > 0 else None\n",
    "\n",
    "def genq_input_prompt(constraints: dict[str:str], context: str):\n",
    "    constraints_text = \"\"\n",
    "    for key, val in constraints.items():\n",
    "        if val and len(val) > 0:\n",
    "            line = _create_constraint_line(val, key)\n",
    "            constraints_text += line if line else \"\"\n",
    "\n",
    "    return _genq_input_prompt.format(constraints=constraints_text, context=context)\n",
    "\n",
    "def genq_messages(constraints: dict[str:str], context: str):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": _genq_system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": genq_input_prompt(constraints, context),\n",
    "        }\n",
    "    ]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70669dcc-992f-478b-a5bf-741c91ef2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class QAPairs(BaseModel):\n",
    "    \"\"\"An object containing generated questions\"\"\"\n",
    "\n",
    "    q: str = Field(\n",
    "        description=\"Generated question\"\n",
    "    )\n",
    "    a: str = Field(\n",
    "        description=\"Generated answer\"\n",
    "    )\n",
    "\n",
    "class GeneratedQA(BaseModel):\n",
    "    \"\"\"An object containing generated question-answer pairs\"\"\"\n",
    "\n",
    "    qa_pairs: list[QAPairs] = Field(\n",
    "        description=\"List of generated question-answer pairs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c58ed-c2ef-4668-99cd-b1185971b26c",
   "metadata": {},
   "source": [
    "### Conjunct extraction\n",
    "We need a function that separates segregatory conjunctions but keep combinatory conjunctions as they become incoherent after separation.\n",
    "We use `spacy` and predefined rules to detect combinatory cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3fd301-31e6-45fa-9e2b-1d861e147ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConjunctsParser:\n",
    "    # Note: These sets might not be complete. Generated by AI and not verified by a linguist\n",
    "    RECIPROCAL_PRONOUNS = {\"each other\", \"one another\"}\n",
    "    MUTUAL_ADVERBS = {\n",
    "        \"together\", \"jointly\", \"mutually\", \"collectively\",\n",
    "        \"reciprocally\", \"conjointly\", \"cooperatively\",\n",
    "        \"collaboratively\", \"interdependently\", \"symbiotically\"\n",
    "    }\n",
    "    COMBINATORY_PHRASES = {\n",
    "        \"as a team\", \"as partners\", \"as a pair\", \"as a group\",\n",
    "        \"hand in hand\", \"side by side\", \"arm in arm\", \"face to face\",\n",
    "        \"in concert\", \"back to back\", \"eye to eye\", \"hand in glove\",\n",
    "        \"shoulder to shoulder\", \"in unison\", \"in league\", \"in collusion\",\n",
    "        \"in partnership\", \"in tandem\", \"in alliance\"\n",
    "    }\n",
    "    CORRELATIVE_KEYWORDS = {\n",
    "        \"between\", \"both\", \"either\", \"neither\", \"whether\"\n",
    "    }\n",
    "\n",
    "    def extract_conjuncts(self, sentence: str, span: spacy.tokens.span.Span):\n",
    "        # Span as str\n",
    "        extracted_str = span.text\n",
    "        doc = nlp(sentence)\n",
    "        \n",
    "        # Case normalization\n",
    "        extracted_lower = extracted_str.lower()\n",
    "        sentence_lower = sentence.lower()\n",
    "\n",
    "        # Rule 1: Reciprocal pronouns in full sentence\n",
    "        if any(pronoun in sentence_lower for pronoun in self.RECIPROCAL_PRONOUNS):\n",
    "            return [extracted_str]\n",
    "\n",
    "        # Rule 2: Combinatory phrases in extracted text\n",
    "        if any(phrase in extracted_lower for phrase in self.COMBINATORY_PHRASES):\n",
    "            return [extracted_str]\n",
    "\n",
    "        # Rule 3: Correlative keywords in extracted text\n",
    "        if any(keyword in extracted_lower for keyword in self.CORRELATIVE_KEYWORDS):\n",
    "            return [extracted_str]\n",
    "\n",
    "        root_verb = next((token for token in doc if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\"), None)\n",
    "        if root_verb:\n",
    "            # Rule 4: Check mutual adverbs modifying the verb\n",
    "            for child in root_verb.children:\n",
    "                if child.lower_ in self.MUTUAL_ADVERBS:\n",
    "                    return [extracted_str]\n",
    "\n",
    "        # Proceed to segregatory splitting if all checks pass\n",
    "        root = span.root\n",
    "        conjunct_heads = [root] + list(root.conjuncts)\n",
    "        conjunct_heads.sort(key=lambda token: token.i)\n",
    "\n",
    "        conjunct_spans = []\n",
    "        for head in conjunct_heads:\n",
    "            conjuncts = []\n",
    "            tokens_in_conjunct = []\n",
    "            for token in head.subtree:\n",
    "                if token in span:\n",
    "                    if token.dep_ == 'cc' and token.pos_ == 'CCONJ':\n",
    "                        if len(tokens_in_conjunct) > 0:\n",
    "                            conjuncts.append(tokens_in_conjunct)\n",
    "                            tokens_in_conjunct = []\n",
    "                    else:\n",
    "                        tokens_in_conjunct.append(token)\n",
    "\n",
    "            if len(tokens_in_conjunct) > 0:\n",
    "                conjuncts.append(tokens_in_conjunct)\n",
    "\n",
    "            for tokens in conjuncts:\n",
    "                start_idx = min(token.i for token in tokens)\n",
    "                end_idx = max(token.i for token in tokens) + 1\n",
    "                conj_span = doc[start_idx:end_idx]\n",
    "                conjunct_spans.append(conj_span.text)\n",
    "\n",
    "        return list(set(conjunct_spans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263283e-202c-46a6-aad2-cf9769b34978",
   "metadata": {},
   "source": [
    "### Semantic relationship and argument extraction\n",
    "We use ClausIE, a clause based information extraction method proposed in:  \n",
    "*ClausIE: Clause-Based Open Information Extraction by Luciano Del Corro and Rainer Gemulla*  \n",
    "The implementation is provided by `spacy-clause` python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb9fed9-e730-4771-a68e-57ae42800e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(text: str):\n",
    "    results = []\n",
    "    doc = nlp(text)\n",
    "    parser = ConjunctsParser()\n",
    "    for sent in doc.sents:\n",
    "        for clause in sent._.clauses:\n",
    "            if 'SV' in clause.type:\n",
    "                extracted = {}\n",
    "                extracted['type'] = clause.type\n",
    "                extracted[\"verb\"] = clause.verb and parser.extract_conjuncts(text, clause.verb)\n",
    "                extracted[\"subject\"] = clause.subject and parser.extract_conjuncts(text, clause.subject)\n",
    "                extracted[\"direct_object\"] = clause.direct_object and parser.extract_conjuncts(text, clause.direct_object)\n",
    "                extracted[\"indirect_object\"] = clause.indirect_object and parser.extract_conjuncts(text, clause.indirect_object)\n",
    "                extracted[\"complement\"] = clause.complement.text if clause.complement else None\n",
    "                results.append(extracted)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f0bb0-14cf-4604-a0d6-7548a4b60dda",
   "metadata": {},
   "source": [
    "### Question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1e207c-d82f-4953-adcf-109c5669c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(inference_provider, constraints_list, context):\n",
    "    questions = []\n",
    "    for constraints in constraints_list:\n",
    "        # This might fail bc of malformed JSON output\n",
    "        # Cloudflare model recommended if fails with HF\n",
    "        result = inference_provider.completition(\n",
    "            model=model,       \n",
    "            messages=genq_messages(constraints, context),\n",
    "            response_schema=GeneratedQA \n",
    "        )\n",
    "        questions += result[\"qa_pairs\"]\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795196d7-2667-4965-a4c3-63ba96f108a8",
   "metadata": {},
   "source": [
    "### Question validation\n",
    "1. We will use a QA model to validate how relevant the answer and the context to the generated questions.  \n",
    "Haystack's ExtractiveReader (roberta distilled model) is used for this task with the default model*.  \n",
    "<sub>*\\* For specific use cases (e.g. medical, biotech) a fine tuned (on application domain) model might be required. Application validation on market data is essential*</sub>\n",
    "3. To decrease redundancy we cluster the validated set based on semantic similarity. The validation score from previous step is used to influence cluster center selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d35231a4-983e-4068-a46c-7ef9c2b8172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from haystack.components.readers import ExtractiveReader\n",
    "from haystack.utils import ComponentDevice, Device\n",
    "import numpy as np\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from operator import itemgetter\n",
    "\n",
    "class QuestionValidator:\n",
    "    def __init__(self, inference_provider: InferenceProviderBase):\n",
    "        device = ComponentDevice.from_single(Device.cpu())\n",
    "        self.reader = ExtractiveReader(device= device)\n",
    "        self.reader.warm_up()\n",
    "        self.inference_provider = inference_provider\n",
    "        self.embedder_model = \"@cf/baai/bge-m3\" \n",
    "\n",
    "    def _run_qa_model(self, question:str, content:str):\n",
    "        res = self.reader.run(\n",
    "            query=question,\n",
    "            documents=[Document(content=content)],\n",
    "            top_k=1\n",
    "        )\n",
    "        if 'answers' in res and len(res['answers']) > 0:\n",
    "            return res['answers'][0].score\n",
    "        return None\n",
    "    \n",
    "    def _validate(self, questions: list[str], context: str, validation_th: float):\n",
    "        valid = []\n",
    "        for qa_pair in questions:\n",
    "            q, a = itemgetter('q', 'a')(qa_pair)\n",
    "            a_score = self._run_qa_model(q, a)\n",
    "            context_score = self._run_qa_model(q, context)\n",
    "            comnined_score = 0.6 * a_score + 0.4 * context_score\n",
    "            if min(a_score, context_score) > validation_th:\n",
    "                valid.append({\"q\": q, \"a\":a, \"score\":comnined_score})\n",
    "        return valid\n",
    "        \n",
    "    def _select_questions(self, valid: dict[str:str], top_k_per_cluster = 2, damping=0.7, max_iter=500):\n",
    "        if len(valid) < 2:\n",
    "            return \n",
    "        \n",
    "        texts = [ f\"Question:{qa['q']} Answer:{qa['a']}\" for qa in valid ]\n",
    "        scores = [ qa[\"score\"] for qa in valid ]\n",
    "        \n",
    "        # Compute embeddings\n",
    "        vectors = self.inference_provider.embedding(self.embedder_model, texts)\n",
    "           \n",
    "        # Compute cosine similarity matrix\n",
    "        similarity_matrix = cosine_similarity(vectors)\n",
    "        \n",
    "        # Scale scores to match similarity range [-1,1] \n",
    "        # to influence cluster center selection\n",
    "        score_min, score_max = min(scores), max(scores)\n",
    "        if score_max == score_min:\n",
    "            scaled_scores = np.zeros(len(scores))\n",
    "        else:\n",
    "            scaled_scores = -1 + 2 * (np.array(scores) - score_min) / (score_max - score_min)\n",
    "        \n",
    "        # Run Affinity Propagation\n",
    "        af = AffinityPropagation(\n",
    "            affinity='precomputed',\n",
    "            preference=scaled_scores,\n",
    "            damping=damping,\n",
    "            max_iter=max_iter,\n",
    "            random_state=100\n",
    "        ).fit(similarity_matrix)\n",
    "        \n",
    "        # Get cluster labels and centers\n",
    "        labels = af.labels_\n",
    "        cluster_centers = af.cluster_centers_indices_\n",
    "                    \n",
    "        # Select cluster centers\n",
    "        sel_indices = [int(c) for c in cluster_centers]\n",
    "        # add items from each cluster if top_k_cluster > 1\n",
    "        if top_k_per_cluster > 1:\n",
    "            clusters = {}\n",
    "            for i, label in enumerate(labels):\n",
    "                if i not in cluster_centers:\n",
    "                    clusters.setdefault(int(label), []).append(i)\n",
    "            for label, indices in clusters.items():\n",
    "                sel_indices += random.sample(indices, k=top_k_per_cluster-1)\n",
    "                \n",
    "        # return selected questions\n",
    "        return [valid[i] for i in sel_indices]\n",
    "        \n",
    "    def validate(self, questions: list[str], context: str, validation_th = 0.7, top_k_per_cluster = 2, damping=0.7, max_iter=500):\n",
    "        valid = self._validate(questions, context, validation_th)\n",
    "        return self._select_questions(valid, top_k_per_cluster, damping, max_iter)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ca99c-202d-4ace-816b-5e925d6efcfc",
   "metadata": {},
   "source": [
    "### Create inference provider instance\n",
    "Also define the text gen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319997b1-be20-44ea-84e4-db07499c834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf_provider - we use cloudflare\n",
    "inference_provider = CFInferenceProvider()\n",
    "model = '@cf/meta/llama-3.1-8b-instruct-fast'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0b2d8-2b2f-4464-8b0e-ec093bc516e6",
   "metadata": {},
   "source": [
    "### Create validator instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "375877c1-922e-4166-9e75-beb505018ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = QuestionValidator(inference_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ee9b0-3d75-4e84-b838-a30611911233",
   "metadata": {},
   "source": [
    "## Test with single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dd102ffa-8f44-4953-b2a4-64cc9fb2fd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'SV',\n",
       "  'verb': ['are fitted'],\n",
       "  'subject': ['photosites'],\n",
       "  'direct_object': None,\n",
       "  'indirect_object': None,\n",
       "  'complement': None},\n",
       " {'type': 'SVC',\n",
       "  'verb': ['means'],\n",
       "  'subject': ['This'],\n",
       "  'direct_object': None,\n",
       "  'indirect_object': None,\n",
       "  'complement': 'some photosites record the intensity of red light, some the intensity of green, and some the intensity of blue'},\n",
       " {'type': 'SVO',\n",
       "  'verb': ['record'],\n",
       "  'subject': ['some photosites'],\n",
       "  'direct_object': ['the intensity of red light'],\n",
       "  'indirect_object': None,\n",
       "  'complement': None},\n",
       " {'type': 'SV',\n",
       "  'verb': ['are passed'],\n",
       "  'subject': ['The electrical signals from all the photosites in the sensor'],\n",
       "  'direct_object': None,\n",
       "  'indirect_object': None,\n",
       "  'complement': None},\n",
       " {'type': 'SVO',\n",
       "  'verb': ['interprets'],\n",
       "  'subject': ['which'],\n",
       "  'direct_object': ['all this information'],\n",
       "  'indirect_object': None,\n",
       "  'complement': None},\n",
       " {'type': 'SVO',\n",
       "  'verb': ['determines'],\n",
       "  'subject': ['which'],\n",
       "  'direct_object': ['brightness values of all the individual pixels (picture elements) that make up a digital image',\n",
       "   'the colour'],\n",
       "  'indirect_object': None,\n",
       "  'complement': None},\n",
       " {'type': 'SVO',\n",
       "  'verb': ['make'],\n",
       "  'subject': ['that'],\n",
       "  'direct_object': ['a digital image'],\n",
       "  'indirect_object': None,\n",
       "  'complement': None}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"In order to capture colours as well as brightness information, photosites are fitted with red, green and blue colour filters. This means some photosites record the intensity of red light, some the intensity of green, and some the intensity of blue. The electrical signals from all the photosites in the sensor are passed to the camera's image processor, which interprets all this information and determines the colour and brightness values of all the individual pixels (picture elements) that make up a digital image.\"\n",
    "constraints_list = extract_information(text)\n",
    "constraints_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "78a192d6-2355-4939-9673-74dbfad2b5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'q': 'What are photosites fitted with',\n",
       "  'a': 'photosites are fitted with red, green and blue colour filters'},\n",
       " {'q': 'How are photosites fitted',\n",
       "  'a': 'photosites are fitted with red, green and blue colour filters'},\n",
       " {'q': 'What does This mean?',\n",
       "  'a': 'This means some photosites record the intensity of red light, some the intensity of green, and some the intensity of blue.'},\n",
       " {'q': 'What does This involve?',\n",
       "  'a': 'This involves some photosites recording the intensity of red light, some the intensity of green, and some the intensity of blue.'},\n",
       " {'q': 'What does This involve in terms of light?',\n",
       "  'a': 'This involves some photosites recording the intensity of red light, some the intensity of green, and some the intensity of blue.'},\n",
       " {'q': 'What does This involve in terms of intensity?',\n",
       "  'a': 'This involves some photosites recording the intensity of red light, some the intensity of green, and some the intensity of blue.'},\n",
       " {'q': 'What does This involve in terms of colour and brightness?',\n",
       "  'a': 'This involves some photosites recording the intensity of red light, some the intensity of green, and some the intensity of blue.'},\n",
       " {'q': 'What does This involve in terms of signals?',\n",
       "  'a': \"This involves the electrical signals from all the photosites in the sensor being passed to the camera's image processor.\"},\n",
       " {'q': 'What do some photosites record?',\n",
       "  'a': 'some photosites record the intensity of red light'},\n",
       " {'q': 'What do some photosites record the intensity of?', 'a': 'red light'},\n",
       " {'q': 'What do some photosites record the intensity of red light for?',\n",
       "  'a': 'some photosites record the intensity of red light'},\n",
       " {'q': 'What do some photosites record the intensity of blue light for?',\n",
       "  'a': 'some photosites record the intensity of blue'},\n",
       " {'q': 'What do some photosites record the intensity of green light for?',\n",
       "  'a': 'some photosites record the intensity of green'},\n",
       " {'q': 'What are the electrical signals from all the photosites in the sensor passed to?',\n",
       "  'a': \"The electrical signals from all the photosites in the sensor are passed to the camera's image processor\"},\n",
       " {'q': 'What are the electrical signals from all the photosites in the sensor passed to determine?',\n",
       "  'a': 'The electrical signals from all the photosites in the sensor are passed to determine the colour and brightness values of all the individual pixels'},\n",
       " {'q': 'What are the electrical signals from all the photosites in the sensor passed to interpret?',\n",
       "  'a': \"The electrical signals from all the photosites in the sensor are passed to the camera's image processor, which interprets all this information\"},\n",
       " {'q': 'What does which interpret?',\n",
       "  'a': 'which interprets all this information'},\n",
       " {'q': 'How does which interpret all this information?',\n",
       "  'a': 'which interprets all this information and determines the colour and brightness values of all the individual pixels'},\n",
       " {'q': 'Which interprets all this information?',\n",
       "  'a': 'which interprets all this information'},\n",
       " {'q': 'What information does which interpret?',\n",
       "  'a': 'which interprets all this information'},\n",
       " {'q': 'What does the image processor interpret?',\n",
       "  'a': 'The image processor interprets all this information'},\n",
       " {'q': 'How does the image processor interpret all this information?',\n",
       "  'a': 'The image processor interprets all this information and determines the colour and brightness values of all the individual pixels'},\n",
       " {'q': \"What does which determine in the camera's image processor?\",\n",
       "  'a': 'which determines the colour and brightness values of all the individual pixels (picture elements) that make up a digital image'},\n",
       " {'q': \"How does the camera's image processor determine which?\",\n",
       "  'a': \"The camera's image processor determines the colour and brightness values of all the individual pixels (picture elements) that make up a digital image\"},\n",
       " {'q': \"What information does the camera's image processor determine from all the photosites?\",\n",
       "  'a': \"The camera's image processor determines the colour and brightness values of all the individual pixels (picture elements) that make up a digital image\"},\n",
       " {'q': 'What does that make up?',\n",
       "  'a': 'a digital image is made up of individual pixels'},\n",
       " {'q': 'How is a digital image made?',\n",
       "  'a': 'a digital image is made up of individual pixels'}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = generate_questions(inference_provider, constraints_list, text)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8618ff74-c971-4f01-800b-ee83db38e657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'q': 'What do some photosites record?',\n",
       "  'a': 'some photosites record the intensity of red light',\n",
       "  'score': 0.8115248918533324},\n",
       " {'q': 'What are the electrical signals from all the photosites in the sensor passed to?',\n",
       "  'a': \"The electrical signals from all the photosites in the sensor are passed to the camera's image processor\",\n",
       "  'score': 0.854365599155426},\n",
       " {'q': \"What information does the camera's image processor determine from all the photosites?\",\n",
       "  'a': \"The camera's image processor determines the colour and brightness values of all the individual pixels (picture elements) that make up a digital image\",\n",
       "  'score': 0.8207377791404724},\n",
       " {'q': 'What does This involve?',\n",
       "  'a': 'This involves some photosites recording the intensity of red light, some the intensity of green, and some the intensity of blue.',\n",
       "  'score': 0.7460584521293641},\n",
       " {'q': 'What are the electrical signals from all the photosites in the sensor passed to determine?',\n",
       "  'a': 'The electrical signals from all the photosites in the sensor are passed to determine the colour and brightness values of all the individual pixels',\n",
       "  'score': 0.8152047753334045},\n",
       " {'q': 'Which interprets all this information?',\n",
       "  'a': 'which interprets all this information',\n",
       "  'score': 0.8121640264987946}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_questions = validator.validate(questions, text, top_k_per_cluster=2)\n",
    "valid_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19202c-8be4-4c7f-9186-badd11f6cb75",
   "metadata": {},
   "source": [
    "## Test context retrival with hybrid search\n",
    "We combine content embedding (for contextual queries) with question embedding (for specific queries).\n",
    "Also use semantic search and term based (attention based with IDF) search with the combination of dense and sparse vectors.  \n",
    "\n",
    "**Article source:**  \n",
    "[Image sensors explained](https://www.canon.hu/pro/infobank/image-sensors-explained/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2328c73-b19f-4f8a-9164-6fb32889af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "Digital imaging basics\n",
    "With all types of sensors, the imaging process begins when light passes through the camera's lens and strikes the sensor. The sensor contains millions of light receptors or photosites, which convert the light energy into an electrical charge. The magnitude of the charge is proportional to the intensity of the light – the more light that hits a particular photosite, the stronger the electrical charge it produces. (SPAD sensors work a little differently – more on this later.)\n",
    "In order to capture colours as well as brightness information, photosites are fitted with red, green and blue colour filters. This means some photosites record the intensity of red light, some the intensity of green, and some the intensity of blue.\n",
    "The electrical signals from all the photosites in the sensor are passed to the camera's image processor, which interprets all this information and determines the colour and brightness values of all the individual pixels (picture elements) that make up a digital image.\n",
    "A diagram showing how a camera creates a digital image, with steps including a mosaic colour filter, an image sensor, an analogue-to-digital converter and the image processor.\n",
    "How cameras create a digital image. Light from the subject you're shooting is focused through the lens onto the image sensor (2), which is covered with a mosaic filter (1) to enable it to detect colour and not just light intensity. The electrical signal generated by the sensor may be amplified by analogue electronics (3) before passing through an analogue-to-digital converter (4) to the image processor (5). After processing, the camera may temporarily hold images in a buffer (6) while it writes them to the memory card.\n",
    "If you're shooting RAW, this data is saved, along with information about the camera settings, in a RAW file. If the camera is set to save images in any other file format – JPEG, HEIF or RAW+JPEG – then further processing takes place in-camera, which typically includes white balance adjustment, sharpening and noise reduction, among other processes, depending on the camera settings. It will also include demosaicing or debayering, which cleverly calculates the correct RGB colour value for each pixel (each individual photosite, remember, records only one colour – red, green or blue). The end result is a complete colour digital image – although, in truth, if the image is a JPEG, more of the original information captured by the sensor has been discarded than has been kept.\n",
    "You conventionally hear about the number of megapixels (millions of pixels) in a sensor, but strictly speaking the sensor does not have pixels at all, but sensels (distinct photosites). What's more, there is not a one-to-one correspondence between sensels in the sensor and pixels in the resulting digital image, for a whole range of technical reasons. It is more accurate to describe a sensor as having a certain number of \"effective pixels\", which simply means that the camera produces images or videos of that number of megapixels. The Canon PowerShot V10, for example, has a sensor described as approximately 20.9MP in \"total pixels\" but some of the sensor data is used for technical processes such as distortion correction and digital image stabilisation, with the result that the PowerShot V10 delivers video (with Movie Digital IS) at approximately 13.1MP and still images (which undergo different processes) at approximately 15.2MP.\n",
    "An illustration of a Bayer array, with alternating rows of red-and-green and blue-and-green colour filters.\n",
    "The most common type of colour filter mosaic in digital sensors, a Bayer array. This is what makes it possible for the sensor to detect colour, not just light intensity. There are more photosites dedicated to green because the human eye happens to be more sensitive to green light than to blue or red.\n",
    "A 1.0-type CMOS sensor.\n",
    "A 1.0-type CMOS sensor. CMOS sensors of this size are used in compact cameras such as the Canon PowerShot G7 X Mark III and video cameras such as the Canon XF605 professional 4K camcorder.\n",
    "CCD sensors\n",
    "There are several different types of image sensor. Digital photography arrived in the mid-1980s with the introduction of CCD (Charge-Coupled Device) sensors. These sensors were the first to make it possible to capture images without the use of film, revolutionising photography.\n",
    "CCD sensors are composed of an integrated grid of semiconductor capacitors capable of holding an electrical charge. When light reaches the sensor, these capacitors, acting as individual photosites, absorb the light and convert it into an electrical charge. The amount of charge at each photosite is directly proportional to the intensity of the light that strikes it.\n",
    "In a CCD sensor, the charge from each photosite is transferred through the sensor's grid (hence the term charge-coupled) and read at one corner of the array, in the same way that water might be passed along a bucket brigade or human chain. This method ensures a high degree of image quality and uniformity because each pixel uses the same pathway to output its signal. For this reason, Canon's first professional digital camera, the EOS-1D, launched in 2001, had a 4.15MP CCD sensor. However, this process is also more power-intensive than the process in CMOS sensors.\n",
    "CMOS sensors\n",
    "In 2000, Canon introduced its first CMOS (Complementary Metal Oxide Semiconductor) sensor, in the 3.1MP EOS D30. Unlike the CCD sensor, which transfers charges across the sensor to a single output node, a CMOS sensor contains multiple transistors at each photosite, enabling the charge to be processed directly at the site. This has several implications.\n",
    "For a start, CMOS sensors require less power, making them more energy efficient. They can also read off electrical charges at a much faster rate, which is crucial for shooting high-speed sequences. What's more, CMOS sensors share the same basic structure as computer microprocessors, which allows for mass production at a lower cost while incorporating additional functions such as noise reduction and image processing right on the sensor.\n",
    "All of Canon's mirrorless EOS R System cameras feature CMOS sensors, as do the EOS DSLR, Cinema EOS and PowerShot camera ranges.\n",
    "A cutaway illustration of Canon's Dual Pixel CMOS AF system.\n",
    "In Canon's Dual Pixel CMOS AF system, each photo receptor in the sensor has two separate photodiodes (marked A and B), and comparing the signals from the two determines whether that point is in sharp focus. At the same time, the output (C) from the photo receptor is used for imaging.\n",
    "A diagram of a sensor with Cross-Type AF points, showing how they are able to detect both vertical lines and horizontal lines.\n",
    "The EOS R1 features a new Dual Pixel CMOS AF sensor arrangement with photodiodes sensitive to phase difference along both the vertical and horizontal axes of the sensor. These Cross Type AF points enable the camera's Dual Pixel Intelligent AF to detect both horizontal and vertical detail, enhancing AF acquisition and tracking performance.\n",
    "Developments in CMOS sensors\n",
    "CMOS sensor technology has continued to evolve. An innovation developed by Canon is Dual Pixel CMOS AF technology, which enables each pixel on the sensor to be used for both imaging and autofocus, resulting in faster and more accurate AF performance.\n",
    "An enhanced version of the system was introduced in 2020: Dual Pixel CMOS AF II. This incorporates EOS intelligent Tracking and Recognition Autofocus (EOS iTR AF X), Canon’s subject detection and tracking system utilising Deep Learning AI. Dual Pixel CMOS AF II is now widely used across the EOS R System and Cinema EOS lines, delivering greater autofocus speed, precision and coverage in stills and video in cameras such as the EOS R7, EOS R6 Mark II and EOS C400.\n",
    "Dual Pixel Intelligent AF, which made its debut in 2024 in the EOS R1 and EOS R5 Mark II, has further refined the detection and tracking capabilities, and enabled the introduction of features such as Action Priority AF, which enables the camera to track actions commonly seen in certain sports and automatically shift the focus to the area where the action is taking place.\n",
    "In the EOS R1, the autofocus is advanced still further with Cross Type AF, enabling the sensor to detect phase difference not just vertically, like other AF systems, but also horizontally at the same time. This enhanced sensitivity results in increased focusing accuracy and speed in low-light and low-contrast situations, and even more stable AF performance in continuous shooting mode.\n",
    "An illustration of the stacked, back-illuminated CMOS sensor in the Canon EOS R3.\n",
    "The stacked, back-illuminated CMOS sensor in the Canon EOS R3 is designed for capturing high-speed and high-resolution imagery.\n",
    "A Canon EOS C400 camera with no lens attached, revealing the sensor.\n",
    "The fast readout of the Back-Side Illuminated (BSI) sensor in the EOS C80 and EOS C400 (shown here) minimises distortion when filming moving objects or when the camera is panned quickly.\n",
    "Another development in Canon's CMOS technology is the stacked, back-illuminated sensor design used in the EOS R1, EOS R5 Mark II and EOS R3. This design places the photodiodes above the transistor layer to improve light collection efficiency, resulting in less image noise and better image quality. Additionally, the stacked structure allows faster data readout, contributing to the camera's high-speed performance.\n",
    "Both the EOS R1 and EOS R5 Mark II are also equipped with a DIGIC Accelerator, which boosts the volume of data that the camera is capable of processing. In combination with the high-speed back-illuminated stacked sensor, the DIGIC Accelerator unlocks a host of features, including faster electronic shutter speeds, simultaneous recording of stills and video, and a significant reduction in rolling shutter distortion compared to earlier cameras.\n",
    "Similar sensor technology is also deployed in selected Cinema EOS cameras. The EOS C80 and EOS C400 incorporate 6K full-frame Back-Side Illuminated (BSI) CMOS sensors, which provide improved low-light performance compared to front-illuminated sensors. As well as delivering 16 stops of dynamic range and minimal noise, the BSI sensor’s fast readout speeds minimise rolling shutter distortion.\n",
    "The Canon ME20F-SH camera with a 50mm EF lens.\n",
    "The Canon ME20F-SH multi-purpose camera can see and shoot in almost complete darkness.\n",
    "The full-frame image sensor from the Canon ME20F-SH ultra-low-light camera.\n",
    "The camera's specially developed full-frame CMOS sensor is designed specifically for low light video capture. With larger photo receptors, it maximises light-gathering capabilities to deliver ultra-low-light images with low noise.\n",
    "Canon's CMOS sensor research and development is ongoing. One result of this is an ultra high sensitivity 35mm full-frame CMOS sensor, with much larger photo receptors (approximately 7.5 times the size of those in previous sensors). Larger photo receptors are able to capture more light, in this case achieving a sensitivity equivalent to ISO 4 million, enabling a camera to capture vivid colour images of very dark environments. This technology is used in the Canon ME20F-SH ultra low light video camera.\n",
    "Canon has also developed an ultra high pixel count sensor, using advanced miniaturisation techniques to reduce the photosite size. This facilitates very high resolution image capture, with a pixel count up to 250MP. In an image captured using this technology, it is possible to distinguish the lettering on an aircraft in flight 18km away and achieve a resolution approximately 30 times higher than that of 4K video. This has great potential for applications in surveillance, astronomical observation and medical imaging.\n",
    "One shortcoming of current CMOS sensors is that, for technical reasons including data bandwidth, their data is read out sequentially rather than all at once. This results in issues such as \"rolling shutter\" distortion of fast-moving subjects that have changed their position during the time the frame is being read out. However, the advanced back-illuminated stacked CMOS sensor design used in cameras such as the EOS R1 and EOS R5 Mark II enables much faster readout speeds, greatly alleviating this issue. Indeed, it is almost completely eliminated in the EOS R1, which boasts a 40% reduction in rolling shutter over the already fast-readout CMOS sensor in the EOS R3.\n",
    "Canon is actively investigating other solutions such as \"global shutter\" technology, which enables readout of the entire sensor in one go, but this technology is very complex, adds both image noise and cost, and can't yet produce very high-quality outputs.\n",
    "A diagram of Canon's Dual Gain Output sensor technology, showing the same image read at two amplification levels and then combined into a single image.\n",
    "The key to Canon's Dual Gain Output (DGO) technology is that each photosite on the sensor is read at two amplification levels, one high-gain and one low, and the two readouts are then combined into a single HDR image with astonishing detail and low noise.\n",
    "The Canon DGO sensor\n",
    "The DGO (Dual Gain Output) sensor is an advanced image sensor used in the Canon EOS C300 Mark III and EOS C70 professional video cameras.\n",
    "Canon’s DGO sensor works by reading each pixel at two different amplification levels, one high and one low, and then combining these two readouts into a single image. The high amplification readout is optimised to capture fine details in shadow regions while reducing noise. The low amplification readout is designed to maintain and accurately reproduce information in the highlights. Combining these produces an image that has a broader dynamic range, retains more detail and exhibits less noise compared to images from conventional sensor technologies.\n",
    "The DGO technology does not consume any more power than a conventional sensor, and is also compatible with Canon's Dual Pixel CMOS AF system and electronic image stabilisation, delivering fast, reliable autofocus and a super-steady image.\n",
    "A diagram comparing the operation of a CMOS sensor with that of a SPAD sensor.\n",
    "Both a CMOS sensor (A) and a SPAD sensor (B) include p -type semiconductors (2) and n -type semiconductors (3) but in different configurations. When a single photon (1) strikes either type of sensor, a single electron is generated (4). In a CMOS sensor, the charge of a single electron is too small to be detected as an electrical signal, so the charge has to be accumulated over a certain period of time. By contrast, a SPAD sensor amplifies the charge by approximately one million times using a phenomenon called Avalanche Multiplication (5), which causes a large current to flow instantaneously, enabling the sensor to detect that a single photon has hit it.\n",
    "The Canon SPAD sensor\n",
    "CCD and CMOS sensors measure the intensity of light – in other words, how many photons reach the sensor within a specified time. SPAD (Single Photon Avalanche Diode) sensors work differently, using the \"avalanche\" effect in semiconductors. When a photon strikes the sensor, it generates an electron, which then triggers a chain reaction or \"avalanche\" of electron production. This cascading effect causes a large current to flow instantaneously, which is read out as a voltage signal in the form of a train of pulses corresponding to individual photons.\n",
    "This unique light-sensing technology means SPAD sensors can achieve incredible low-light performance. Using the outstanding SPAD sensor, Canon has developed the MS-500, a breakthrough interchangeable-lens camera capable of capturing high-definition colour footage in extremely low-light conditions, even the near-total darkness of a night-time environment.\n",
    "In addition, the MS-500's bayonet mount for a 2/3-inch broadcast lens enables the camera to utilise Canon's extensive range of broadcast lenses, with their excellent super-telephoto optical performance. This means the camera is able to resolve subjects several kilometres away, even if they are unlit, making it an invaluable asset for security, surveillance and a broad range of scientific applications.\n",
    "CINC_Product_H264\n",
    "Sensor sizes explained\n",
    "It's clear that a sensor's megapixel count (whether it's total or effective pixels) isn't the whole story. The physical size of the sensor is an important factor. APS-C sensors are physically smaller than full-frame sensors, which means that even if the pixel counts are identical, a camera with a full-frame sensor should deliver a wider dynamic range and better low-light performance – if it has the same megapixel count but over a larger area, then it has larger photosites, which will be capable of capturing more light. This makes full-frame cameras such as the EOS R1 and EOS R5 Mark II a favourite choice for professionals, particularly those shooting landscapes, architecture or portraits.\n",
    "Conversely, because APS-C sensors are smaller, your subject will fill more of the frame than it would if you used the same lens with the same settings on a full-frame camera – so in effect, an APS-C sensor increases the reach of your lens. In Canon cameras, the \"crop factor\" is approximately 1.6x, giving you an effective focal length 1.6x greater than the same lens on a full-frame camera. This gives a 50mm lens, for example, the field of view of an 80mm lens (50 x 1.6 = 80). This means APS-C cameras are well suited for a broad range of uses including wildlife and street photography. In addition, thanks to the smaller sensor, APS-C cameras such as the EOS R50 and EOS R10 are smaller and lighter than their full-frame counterparts, making them a great option for travel or nature shoots.\n",
    "Some video cameras use Super 35mm sensors (active area approximately 24.6 x 13.8mm, depending on the resolution setting), which are slightly larger than APS-C (22.2 x 14.8mm) but still less than half the area of full-frame (36 x 24mm). They are widely used in the film industry thanks to their balance between cost, image quality and cinematic look (with a shallow depth of field). Camcorders and other camera types use a range of other sensor sizes, such as the 20.1MP 1.0-type stacked CMOS sensor in the compact PowerShot G7 X Mark III and the 11.7MP 1/2.3 CMOS sensor in the PowerShot PX.\n",
    "An APS-C sensor in front of a full-frame sensor, showing their relative sizes.\n",
    "CMOS sensors come in different sizes. A full-frame sensor has approximately 1.6x the active surface area of an APS-C sensor.\n",
    "A diagram showing a \"light bucket\" containing yellow photons and grey noise, alongside a larger one with more yellow photons.\n",
    "If two sensors have the same total pixel count but one is physically larger than the other, then each photosite on the larger one must be bigger. This is sometimes included in camera specs as the \"pixel pitch\" – a 21MP APS-C camera might have a pixel pitch of about 4.22 microns while a 21MP full-frame camera might be 6.45 microns. Photosites act as \"light buckets\" and, in the same way that a wider bucket would capture more rainwater than a narrower bucket, a larger photosite captures more photons (shown in yellow) with relatively less random noise (grey).\n",
    "The choice of sensor size depends largely on your shooting requirements and budget. Each sensor size offers distinct advantages, and understanding these can help you select the right camera for your specific needs. However, you can see why standardising on \"effective pixels\" provides a simpler measure for comparing different cameras and different technologies!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096c8e3-e212-4ba2-8889-ce71db04b46b",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "We will use multiple embeddings. Hybrid retrieval leverages the strengths of various methods, offering higher accuracy and stronger generalization capabilities\n",
    "1. Dense retrieval: maps the text into a single embedding. There are many available text embedding models.\n",
    "2. Sparse retrieval (lexical matching): a vector of size equal to the vocabulary, with the majority of positions set to zero, calculating a weight only for tokens present in the text. e.g., BM25. In this case we will use BM42 that replaces term frequencí with attention weights\n",
    "3. Multi-vector retrieval: use multiple vectors to represent a text. ColBERT.\n",
    "\n",
    "BAAI/bge-m3 can generate all 3 vectors in one shot but currently the available inference endpoint only generates dense embedding. It is also a multi-lingual model ideal for international applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fbf1d-de4d-4bcc-a017-5e8d857475cb",
   "metadata": {},
   "source": [
    "#### Dense embedding\n",
    "We will use BAAI/bge-m3 from Cloudflare Workers AI.  \n",
    "The previously implemented CFInferenceProvider already supports embedding inference endponts.  \n",
    "*The model will run on the local machine but it is relative small so should not be a problem on CPU.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6d34a-68a1-4414-8eb5-3ca09d68e623",
   "metadata": {},
   "source": [
    "#### Sparse embedding\n",
    "We will use the BM42 embedder that uses attention weights representing term importance instead of term frequency.  \n",
    "`fastenbed` pacjage implements this method as SparseTextEmbedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c67c7d-eca4-4f36-a51f-911e2b33cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import LateInteractionTextEmbedding, SparseTextEmbedding\n",
    "\n",
    "class BM42Embedder:\n",
    "    def __init__(self):\n",
    "        self.model = SparseTextEmbedding(\"Qdrant/bm42-all-minilm-l6-v2-attentions\")\n",
    "    def embeddding(self, texts: list[str]):\n",
    "        embeddings = self.model.embed(texts)        \n",
    "        return list(map(lambda e: e.as_object(), embeddings))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44560cd-ebff-4653-88fd-9a6d62cc1c2e",
   "metadata": {},
   "source": [
    "#### Late interaction embedding\n",
    "For multi-vector embedding we will use ColBERT. `fastenbed` pacjage implements this method as LateInteractionTextEmbedding.  \n",
    "*The model will run on the local machine but it is relative small so should not be a problem on CPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61314026-02d9-40de-88ae-eed6bca39dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColBERTV2:\n",
    "    def __init__(self):\n",
    "        self.model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
    "    def embeddding(self, texts: list[str]):\n",
    "        embeddings = self.model.embed(texts)\n",
    "        return list(map(lambda e: e.tolist(), embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996f77c-d061-4960-b179-48ddba0076e9",
   "metadata": {},
   "source": [
    "#### Multi Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e981e28-c25c-48ae-b33d-72333c6a060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, inference_provider):        \n",
    "        self.inference_provider = inference_provider\n",
    "        self.dense_model = \"@cf/baai/bge-m3\"\n",
    "        self.sparse = BM42Embedder()\n",
    "        self.colbert = ColBERTV2()    \n",
    "    def embedding(self, texts):\n",
    "        dense = self.inference_provider.embedding(self.dense_model, texts)\n",
    "        sparse = self.sparse.embeddding(texts)\n",
    "        colbert = self.colbert.embeddding(texts)\n",
    "        return [{            \n",
    "            \"dense\": vec,\n",
    "            \"sparse\": sparse[i],\n",
    "            \"colbert\": colbert[i],\n",
    "        } for (i, vec) in enumerate(dense) ]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ce9f7-e220-4ac5-b183-2210e5936b6f",
   "metadata": {},
   "source": [
    "#### Reranker\n",
    "It will rerank the dual results. Results from document search and QA search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bf9fce2-5546-474d-b2f8-5ec54f7ca57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reranker:\n",
    "    def __init__(self, inference_provider):        \n",
    "        self.inference_provider = inference_provider\n",
    "        self.model = \"@cf/baai/bge-m3\"\n",
    "    def rank(self, query, contexts, min_score=0.7, top_k=10):\n",
    "        return inference_provider.rerank(\n",
    "            self.model,\n",
    "            query, \n",
    "            [{\"text\": context} for context in contexts],\n",
    "            top_k\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cd28e-27f0-4e00-9f9c-825ce42aebd2",
   "metadata": {},
   "source": [
    "## Index the article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58efa3-46a1-4eca-9689-cfd22cab4c6c",
   "metadata": {},
   "source": [
    "### Create Vector DB\n",
    "We will use Qdrant as it has great support for hybrid vector search including sparse vectors wit built in IDF calulation.\n",
    "Qdrant is open source one can run locally as container or deploy to any Cloud provider. We will use the free managed service for development and testing. The notebook will need a QDRANT_URL and a QDRANT_API_KEY environment varible to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05200f08-d729-4eaf-81f6-6adc5de3d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# Utility class for store split text\n",
    "@dataclass\n",
    "class TextPart():\n",
    "    id: str\n",
    "    content: int\n",
    "    source_id: int # e.g. article id \n",
    "\n",
    "# Utility class for questions\n",
    "@dataclass\n",
    "class Question():\n",
    "    id: str\n",
    "    document: str\n",
    "    q: str\n",
    "    a: str\n",
    "\n",
    "# Utility class for context\n",
    "@dataclass\n",
    "class Context():\n",
    "    content: str\n",
    "    score: float\n",
    "    document_id: str\n",
    "    question: str = None\n",
    "    answer: str = None\n",
    "\n",
    "def join_qa(q:str, a: str, sep=' '):\n",
    "    return f\"Question: {q}{sep}Answer: {a}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dc276242-d906-42d7-a3ad-0c1ce6935ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import models\n",
    "from typing import Union\n",
    "\n",
    "class VectorDB:\n",
    "    DENSE_LENGTH = 1024\n",
    "    COLBERT_LENGTH = 128\n",
    "    COLLECTION_DOCUMENTS = 'documents'\n",
    "    COLLECTION_QUESTIONS = 'questions'\n",
    "    \n",
    "    def __init__(self, embedder:Embedder, reranker:Reranker):\n",
    "        self.client = QdrantClient(url=os.getenv('QDRANT_URL'), api_key=os.getenv('QDRANT_API_KEY'))\n",
    "        self.embedder = embedder\n",
    "        self.reranker = reranker\n",
    "        vectors_config = {\n",
    "            \"dense\": models.VectorParams(size=self.DENSE_LENGTH, distance=models.Distance.COSINE),\n",
    "            \"colbert\": models.VectorParams(\n",
    "                size=self.COLBERT_LENGTH,\n",
    "                distance=models.Distance.COSINE,\n",
    "                multivector_config=models.MultiVectorConfig(\n",
    "                    comparator=models.MultiVectorComparator.MAX_SIM,\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "        sparse_vectors_config = {                \n",
    "            \"sparse\": models.SparseVectorParams(modifier=models.Modifier.IDF)                \n",
    "        }\n",
    "        # Full text index for document content\n",
    "        text_search_schema = models.TextIndexParams(\n",
    "            type=models.PayloadSchemaType.TEXT,\n",
    "            tokenizer=models.TokenizerType.MULTILINGUAL,\n",
    "            lowercase=True\n",
    "        )\n",
    "        if not self.client.collection_exists(collection_name=self.COLLECTION_DOCUMENTS):\n",
    "            # Doc id vill be the point id we dont need a separate id for\n",
    "            self.client.create_collection(\n",
    "                self.COLLECTION_DOCUMENTS,\n",
    "                vectors_config=vectors_config,\n",
    "                sparse_vectors_config=sparse_vectors_config\n",
    "            )                        \n",
    "            # Doc content\n",
    "            self.client.create_payload_index(\n",
    "                collection_name=self.COLLECTION_DOCUMENTS,\n",
    "                field_name='content',\n",
    "                field_schema=text_search_schema,\n",
    "            )\n",
    "        if not self.client.collection_exists(collection_name=self.COLLECTION_QUESTIONS):\n",
    "            # Question id vill be the point id we dont need a separate id for\n",
    "            self.client.create_collection(\n",
    "                self.COLLECTION_QUESTIONS,\n",
    "                vectors_config=vectors_config,\n",
    "                sparse_vectors_config=sparse_vectors_config\n",
    "            )            \n",
    "            # Ppayload index for doc id (KEYWORD for strings)\n",
    "            self.client.create_payload_index(\n",
    "                collection_name=self.COLLECTION_QUESTIONS,\n",
    "                field_name='document_id',\n",
    "                field_schema=models.PayloadSchemaType.KEYWORD,\n",
    "            )\n",
    "            # QA content\n",
    "            for field in [\"q\", \"a\"]:\n",
    "                self.client.create_payload_index(\n",
    "                    collection_name=self.COLLECTION_QUESTIONS,\n",
    "                    field_name=field,\n",
    "                    field_schema=text_search_schema,\n",
    "                )\n",
    "    def _check_existing(self, data: list, collection: str):        \n",
    "        ids = [d.id for d in data]\n",
    "        res = self.client.scroll(\n",
    "            collection_name=collection,\n",
    "            scroll_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.HasIdCondition(has_id=ids)\n",
    "                ]\n",
    "            ),\n",
    "            limit=1000,\n",
    "            with_payload=False,\n",
    "            with_vectors=False,\n",
    "        )\n",
    "        stored = [rec.id for rec in res[0]] if len(res) > 0 else []\n",
    "        missing_ids = set(ids) - set(stored)\n",
    "        return [d for d in data if d.id in missing_ids]\n",
    "    def add_documents(self, documents: list[TextPart]):\n",
    "        documents_to_index = self._check_existing(documents, self.COLLECTION_DOCUMENTS)\n",
    "        texts = [doc.content for doc in documents_to_index]\n",
    "        embeddings = self.embedder.embedding(texts)\n",
    "        points = [            \n",
    "            models.PointStruct(\n",
    "                id=documents_to_index[i].id,\n",
    "                payload= {\n",
    "                    \"content\": documents_to_index[i].content\n",
    "                },\n",
    "                vector=vector\n",
    "            ) for (i, vector) in enumerate(embeddings) \n",
    "        ]\n",
    "        self.client.upsert(\n",
    "            collection_name=self.COLLECTION_DOCUMENTS,\n",
    "            points=points\n",
    "        )\n",
    "    def add_questions(self, questions: list[Question]):\n",
    "        qa_to_index = self._check_existing(questions, self.COLLECTION_QUESTIONS)\n",
    "        texts = [join_qa(qa.q, qa.a) for qa in qa_to_index]\n",
    "        embeddings = self.embedder.embedding(texts)\n",
    "        points = [            \n",
    "            models.PointStruct(\n",
    "                id=qa_to_index[i].id,\n",
    "                payload= {\n",
    "                    \"document_id\": qa_to_index[i].document,\n",
    "                    \"q\": qa_to_index[i].q,\n",
    "                    \"a\": qa_to_index[i].a\n",
    "                },\n",
    "                vector=vector\n",
    "            ) for (i, vector) in enumerate(embeddings) \n",
    "        ]\n",
    "        self.client.upsert(\n",
    "            collection_name=self.COLLECTION_QUESTIONS,\n",
    "            points=points\n",
    "        )\n",
    "    def get_documents_by_id(self, ids: list[str]):\n",
    "        res = self.client.scroll(\n",
    "            collection_name=self.COLLECTION_DOCUMENTS,\n",
    "            scroll_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.HasIdCondition(has_id=ids)\n",
    "                ]\n",
    "            ),\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "        )\n",
    "        return res[0] if len(res) > 0 else []\n",
    "    def keyword_query(\n",
    "            self,\n",
    "            keywords: list[str],\n",
    "            limit=10,\n",
    "        ):\n",
    "        field_conditions = [\n",
    "            models.FieldCondition(\n",
    "                key=\"content\",\n",
    "                match=models.MatchText(text=keyword),\n",
    "            ) for keyword in keywords\n",
    "        ]\n",
    "\n",
    "        points = self.client.scroll(\n",
    "            collection_name=self.COLLECTION_DOCUMENTS,\n",
    "            scroll_filter=models.Filter(\n",
    "                # should represents the OR operator\n",
    "                should=field_conditions\n",
    "            ),\n",
    "            with_payload=True,\n",
    "            limit=limit,\n",
    "        )\n",
    "        return points[0]\n",
    "    def hybrid_query(\n",
    "        self,\n",
    "        query_text: str,       \n",
    "        limit=10,\n",
    "    ):\n",
    "        embeddings = self.embedder.embedding([query_text])\n",
    "        prefetch= [\n",
    "            models.Prefetch(\n",
    "                query=embeddings[0][\"dense\"],\n",
    "                using=\"dense\",\n",
    "                limit=10,\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=embeddings[0][\"sparse\"],\n",
    "                using=\"sparse\",\n",
    "                limit=10,\n",
    "            ),\n",
    "        ]\n",
    "        doc_results = self.client.query_points(\n",
    "            collection_name=self.COLLECTION_DOCUMENTS,\n",
    "            prefetch=prefetch,\n",
    "            query=embeddings[0][\"colbert\"],\n",
    "            using=\"colbert\",\n",
    "            with_payload=True,\n",
    "            limit=10,\n",
    "        )\n",
    "        question_results = self.client.query_points(\n",
    "            collection_name=self.COLLECTION_QUESTIONS,\n",
    "            prefetch=prefetch,\n",
    "            query=embeddings[0][\"colbert\"],\n",
    "            using=\"colbert\",\n",
    "            with_payload=True,\n",
    "            limit=10,\n",
    "        )\n",
    "        contexts = []\n",
    "        for point in doc_results.points:\n",
    "            contexts.append({\n",
    "                \"id\": point.id,\n",
    "                \"type\": \"doc\",\n",
    "                \"content\": point.payload[\"content\"],\n",
    "                \"document_id\": point.id\n",
    "            })\n",
    "        for point in question_results.points:\n",
    "            q_c = {\n",
    "                \"id\": point.id,\n",
    "                \"q\": point.payload[\"q\"],\n",
    "                \"a\": point.payload[\"a\"],\n",
    "                \"content\": point.payload[\"q\"],\n",
    "                \"document_id\": point.payload[\"document_id\"]\n",
    "            }\n",
    "            contexts.append(q_c)\n",
    "            qa_c = q_c.copy()\n",
    "            qa_c.update({\n",
    "                \"content\": join_qa(point.payload[\"q\"], point.payload[\"a\"]),\n",
    "            })\n",
    "            contexts.append(qa_c)            \n",
    "            \n",
    "        contents =  [c[\"content\"] for c in contexts]\n",
    "        ranked = self.reranker.rank(query_text, contents)       \n",
    "        \n",
    "        # Prepare final context\n",
    "        return_contexts = []\n",
    "        doc_ids = set()                \n",
    "        for r in ranked[:limit]:\n",
    "            id, score = itemgetter('id', 'score')(r)\n",
    "            c = contexts[id]\n",
    "            doc_id = c[\"document_id\"]\n",
    "            doc_ids.add(doc_id)\n",
    "            if 'q' in c:                \n",
    "                return_contexts.append(\n",
    "                    Context(\n",
    "                        document_id=doc_id, \n",
    "                        content=None, \n",
    "                        score=score, \n",
    "                        question=c[\"q\"], \n",
    "                        answer=c[\"a\"]\n",
    "                    )\n",
    "                )\n",
    "        points = self.get_documents_by_id(doc_ids)\n",
    "        document_contents = { point.id:point.payload[\"content\"] for point in points }\n",
    "        return_contexts += [ \n",
    "            Context(document_id=id, content=document_contents[id], score=score) \n",
    "            for id in doc_ids\n",
    "        ]    \n",
    "        return_contexts.sort(key=lambda c: c.score, reverse=True)\n",
    "        return return_contexts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a187b49-b9cc-44d3-af1d-a8af1630bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = VectorDB(Embedder(inference_provider), Reranker(inference_provider))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a38d85-ccff-481d-bf4d-3baee2a0ce5f",
   "metadata": {},
   "source": [
    "### Split the article into smaller texts\n",
    "We need embeddings to balance capturing specificity and retaining context. Small chunks provide specificity larger chunks retain more context.\n",
    "Our question extraction method excelent for specificity as it works on the sentence level therefore we can use larger chunks to get more context and semantic relationships between sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6931cf3-83d0-4589-962f-af60fd6f76b2",
   "metadata": {},
   "source": [
    "#### Split to large blocks initially using simple rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cac66458-a35a-4247-8e35-285f6df59a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.components.preprocessors.sentence_tokenizer:No abbreviations file found for en. Using default abbreviations.\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.preprocessors import DocumentPreprocessor\n",
    "\n",
    "# we have one article\n",
    "doc_list = [Document(content=article)]\n",
    "\n",
    "preprocessor = DocumentPreprocessor(\n",
    "    split_by=\"word\", \n",
    "    split_length=450, \n",
    "    split_overlap=40, \n",
    "    split_threshold=350,\n",
    "    respect_sentence_boundary=True,    \n",
    ")\n",
    "large_documents = preprocessor.run(documents=doc_list)['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e3728-0cca-403f-b8ed-5bbe6048ac99",
   "metadata": {},
   "source": [
    "#### Split to smaller blocks with a language model\n",
    "To preserve semantic context and technical explanations we will use a language model to further split the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff91ff42-98ba-4cb9-836d-4a58213b91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_split_system_prompt = \"\"\"\n",
    "Analise the input text and split to preserve semantic context and technical explanations.  \n",
    "\n",
    "Rules:\n",
    "1. preserve semantic context, technical term and explanations\n",
    "2. Number of chunks between 2 and 4. \n",
    "3. Do not rephrase, add or omit text.\n",
    "4. Do not add notes or explain.\n",
    "5. Do not add titles, summaries, etc..\n",
    "6. Strictly follow the JSON output format.\n",
    "\n",
    "Example:\n",
    "Text:\n",
    "The Maldives and luxury go hand in hand. This is a country made up almost entirely of privately owned islands, \n",
    "many of which have been turned into indulgent resorts: palm-thatched overwater villas, seafood feasts o the beach, \n",
    "day-tripping to deserted islands for snorkelling and secluded sunbathing. All-inclusive holidays are very popular \n",
    "on the islands thanks to its remoteness. You can just kick back, relax and enjoy the wonderful scenery around you. \n",
    "The Maldives is also great for diving — its waters are filled with whale sharks, manta rays, turtles and coral reefs.\n",
    "\n",
    "---\n",
    "Output:\n",
    "{\n",
    "    \"chunks\": [\n",
    "        \"The Maldives and luxury go hand in hand. This is a country made up almost entirely of privately owned islands, many of which have been turned into indulgent resorts: palm-thatched overwater villas, seafood feasts on the beach, day-tripping to deserted islands for snorkelling and secluded sunbathing.\",\n",
    "        \"All-inclusive holidays are very popular on the islands thanks to its remoteness. You can just kick back, relax and enjoy the wonderful scenery around you. The Maldives is also great for diving — its waters are filled with whale sharks, manta rays, turtles and coral reefs.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "_split_input_prompt = \"\"\"\n",
    "Split the text strictly following the rules and output JSON format!\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def split_text_messages(text: str):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": _split_system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": _split_input_prompt.format(text=text),\n",
    "        }\n",
    "    ]  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55837f29-94e3-4595-9a57-7ed86f8d1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The response format's pydantic model\n",
    "class SplitText(BaseModel):\n",
    "    \"\"\"An object containing the split text\"\"\"\n",
    "\n",
    "    chunks: list[str] = Field(\n",
    "        description=\"List of generated text chunks\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c69bb11-ce74-43ee-a2dd-af0a93da3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import hashlib\n",
    "from json import JSONEncoder\n",
    "\n",
    "# Utility function to generate id from string\n",
    "def str_to_uuid(value: str):\n",
    "    return str(uuid.UUID(hex=hashlib.md5(value.encode(\"UTF-8\")).hexdigest()))\n",
    "\n",
    "def deduplicate_docs(documents):    \n",
    "    return list({ doc.id:doc for doc in documents }.values())\n",
    "\n",
    "# Writing to file\n",
    "def _default(o):    \n",
    "    if hasattr(o, \"__dict__\"):\n",
    "        return o.__dict__\n",
    "    return o\n",
    "def store_data(data, path):\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(data, file, indent=2, default=_default)   \n",
    "\n",
    "# Load data from a file\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return json.load(file)    \n",
    "\n",
    "def list_reader(data, target_class):\n",
    "    return [ target_class(**val) for val in data ]\n",
    "\n",
    "def dict_reader(data, target_class):\n",
    "    return { key:target_class(**val) for key, val in data }\n",
    "\n",
    "# Filenames\n",
    "doc_path = \"documents.json\"\n",
    "questions_path = \"questions.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d01e3-60c1-4400-99c2-438689607e8e",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\"documents.json\" file available to skip the followin cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f8b4373-859c-448b-bbc5-a28debc95492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run language model to split large_documents\n",
    "documents = []\n",
    "for doc in large_documents:\n",
    "    result = inference_provider.completition(\n",
    "        model=model,       \n",
    "        messages=split_text_messages(doc.content),\n",
    "        response_schema=SplitText \n",
    "    )\n",
    "    documents += [ TextPart(id=str_to_uuid(chunk), content=chunk, source_id=doc.meta[\"source_id\"]) for chunk in result[\"chunks\"] ]\n",
    "\n",
    "# Because of the overlap in the original spliting we can have duplicates\n",
    "documents = deduplicate_docs(documents)\n",
    "# write documents to file\n",
    "store_data(documents, doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ea606f2-82d3-413b-a354-11b04cd6eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "data = load_data(doc_path)\n",
    "documents = list_reader(data, TextPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117cabd6-ec68-4ddc-b3b7-fba00723d141",
   "metadata": {},
   "source": [
    "### Create Embeddings for the documents and store them in Vector DB\n",
    "Embedder is passed ot the vector DB class, it processes the documents and saves the vectors to the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd125c5a-831a-40b2-b856-44d383f4bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:21<00:00,  7.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process in batches \n",
    "batch_size = 20\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    batch = documents[i:i + batch_size]\n",
    "    vector_db.add_documents(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47cd85-5681-476a-a21a-953924ba8702",
   "metadata": {},
   "source": [
    "### Create embeddings for the questions and store them in Vector DB\n",
    "The following cell will run for a while. Uses LLM inference and validates questions with a local model runtime depends on the machine runing this notebook.\n",
    "\n",
    "**Note**: \"questions.json\" file available to skip the following 2 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4a124b6-a421-486b-a147-03537c2e204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init list here so we can retry in case of failure without starting over\n",
    "questions_by_doc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "521a4a7a-57dc-4d3e-9abb-3bda50e5b631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 59/59 [00:00<00:00, 265519.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    # in case of a failure we can retry without restarting    \n",
    "    if doc.id not in questions_by_doc:\n",
    "        constraints_list = extract_information(doc.content)\n",
    "        raw_questions = generate_questions(inference_provider, constraints_list, doc.content)\n",
    "        valid = validator.validate(raw_questions, context=doc.content, top_k_per_cluster=2)\n",
    "        group = []\n",
    "        for qa_pair in valid or []:\n",
    "            q, a = itemgetter('q', 'a')(qa_pair)   \n",
    "            q_id = str_to_uuid(join_qa(q, a))\n",
    "            group.append(Question(id=q_id, document=doc.id, q=q, a=a))\n",
    "        questions_by_doc[doc.id] = group\n",
    "\n",
    "# flatten the dict we have the doc id dont need them groupped\n",
    "questions = reduce(lambda a, val: a + val, questions_by_doc.values(), [])\n",
    "# write questions to file\n",
    "store_data(questions, questions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90616e21-212f-4002-9f95-8af192376928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "data = load_data(questions_path)\n",
    "questions = list_reader(data, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c9f98-72d9-42a2-bed0-17f8112bd81e",
   "metadata": {},
   "source": [
    "### Create embeddings for the questions and store them in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49ac585c-26c1-4bea-b481-5f556007cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 64403.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process in batches \n",
    "batch_size = 40\n",
    "for i in tqdm(range(0, len(questions), batch_size)):\n",
    "    batch = questions[i:i + batch_size]\n",
    "    vector_db.add_questions(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f132d03-cef1-4205-9036-eee4b3e7473b",
   "metadata": {},
   "source": [
    "### Run the dual-scale hybrid query\n",
    "It is dual-scale because combines fine-grained QA pairs + contextual chunks. Hybrid because uses dense + sparse embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0d66b30c-9cae-4b1c-9a83-539178cbcf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_text = \"What happens when light enters the sensor?\"\n",
    "query_text = \"Where is relevant the avalanche of electron production?\"\n",
    "contexts = vector_db.hybrid_query(query_text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "776cc065-dad4-4cd7-9014-497b81e89c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print context\n",
    "def build_context_prompt(contexts):\n",
    "    context_prompt = ''\n",
    "    for c in contexts:\n",
    "        if c.question:\n",
    "            context_prompt += \"Related QA:\\n\"\n",
    "            context_prompt += f\"Q: {c.question}\\n\"\n",
    "            context_prompt += f\"A: {c.answer}\\n\\n\"\n",
    "        else:\n",
    "            context_prompt += \"Related content:\\n\"\n",
    "            context_prompt += f\"{c.content}\\n\\n\"\n",
    "    return context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3c933b93-37f1-42a1-8507-bda73942a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related QA:\n",
      "Q: What type of sensors work using the 'avalanche' effect in semiconductors?\n",
      "A: SPAD (Single Photon Avalanche Diode) sensors work using the 'avalanche' effect in semiconductors\n",
      "\n",
      "Related QA:\n",
      "Q: What results from the phenomenon of Avalanche Multiplication?\n",
      "A: Avalanche Multiplication causes a large current to flow instantaneously\n",
      "\n",
      "Related QA:\n",
      "Q: How is a single electron generated?\n",
      "A: When a single photon strikes either type of sensor, a single electron is generated\n",
      "\n",
      "Related QA:\n",
      "Q: What do SPAD sensors work using?\n",
      "A: the avalanche effect in semiconductors\n",
      "\n",
      "Related QA:\n",
      "Q: What results from the phenomenon of Avalanche Multiplication?\n",
      "A: Avalanche Multiplication causes a large current to flow instantaneously\n",
      "\n",
      "Related content:\n",
      "Both a CMOS sensor (A) and a SPAD sensor (B) include p -type semiconductors (2) and n -type semiconductors (3) but in different configurations. When a single photon (1) strikes either type of sensor, a single electron is generated (4). In a CMOS sensor, the charge of a single electron is too small to be detected as an electrical signal, so the charge has to be accumulated over a certain period of time.\n",
      "\n",
      "Related content:\n",
      "The Canon SPAD sensor CCD and CMOS sensors measure the intensity of light – in other words, how many photons reach the sensor within a specified time. SPAD (Single Photon Avalanche Diode) sensors work differently, using the \"avalanche\" effect in semiconductors. When a photon strikes the sensor, it generates an electron, which then triggers a chain reaction or \"avalanche\" of electron production. This cascading effect causes a large current to flow instantaneously, which is read out as a voltage signal in the form of a train of pulses corresponding to individual photons.\n",
      "\n",
      "Related content:\n",
      "When a single photon (1) strikes either type of sensor, a single electron is generated (4). In a CMOS sensor, the charge of a single electron is too small to be detected as an electrical signal, so the charge has to be accumulated over a certain period of time. By contrast, a SPAD sensor amplifies the charge by approximately one million times using a phenomenon called Avalanche Multiplication (5), which causes a large current to flow instantaneously, enabling the sensor to detect that a single photon has hit it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(build_context_prompt(contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c91f08-3051-434b-a4cb-a225dec496ca",
   "metadata": {},
   "source": [
    "### Try a RAG (Retrival Augmented Generation) example with the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "855adb66-84a4-47bc-8b57-1448c51d2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rag_sys_prompt = _split_input_prompt = \"\"\"\n",
    "You are a helpful assistant answering questions based on the provided context!\n",
    "Ensure:\n",
    "    - Consider the context in your answer, can use the phrases from the context\n",
    "    - The context shall be transparent for the end user. Do NOT reference the context in any way in cluding \"based on the context\"\n",
    "    - If the context is highly irrelevant ignor\n",
    "    - If the context seems contradictory try to resolve the contradiction otherwise ignore the context\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "def rag_messages(query: str, context:str):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": _rag_sys_prompt.format(context=context),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7ee1f-c5bd-4b05-9f47-1ad0f38c5d1b",
   "metadata": {},
   "source": [
    "With relevant context we can use smaller and more cost effective LLMs. In this example we are using a model with 8B parameters.  \n",
    "These smaller models are also more accessible to fine tune and self host by businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f3f4aa38-53c2-4c2b-86ee-2172abc70f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avalanche of electron production occurs in the SPAD (Single Photon Avalanche Diode) sensor. When a photon strikes the SPAD sensor, it generates an electron, which then triggers a chain reaction or \"avalanche\" of electron production, causing a large current to flow instantaneously.\n"
     ]
    }
   ],
   "source": [
    "assistants_answer = inference_provider.completition(\n",
    "    model=model,       \n",
    "    messages=rag_messages(query_text, build_context_prompt(contexts)),\n",
    ")\n",
    "print(assistants_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3dd24fdb-4d00-4d3b-81ad-30c800f23e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In various astrophysical and laboratory settings, the avalanche of electron production can be relevant.\n",
      "\n",
      "In high-energy astrophysics, the avalanche of electron production can occur in the presence of intense magnetic fields and high-energy particle interactions, such as in the vicinity of neutron stars, black holes, or during supernova explosions.\n",
      "\n",
      "In laboratory settings, the avalanche of electron production can be induced through various methods, including high-powered laser interactions with matter, particle accelerators, or high-energy electron beam experiments.\n"
     ]
    }
   ],
   "source": [
    "assistants_answer_no_context = inference_provider.completition(\n",
    "    model=model,       \n",
    "    messages=rag_messages(query_text, 'No available context'),\n",
    ")\n",
    "print(assistants_answer_no_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-gen",
   "language": "python",
   "name": "question-gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
